version: "3.9"

services:
  db:
    image: postgres:15
    environment:
      POSTGRES_USER: visionbi
      POSTGRES_PASSWORD: visionbi
      POSTGRES_DB: visionbi
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  tgi:
    image: ghcr.io/huggingface/text-generation-inference:2.0.4
    environment:
      - MODEL_ID=${MODEL_ID:-meta-llama/Meta-Llama-3.1-8B-Instruct}
      - HF_HOME=/data/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
    command: >-
      --model-id ${MODEL_ID:-meta-llama/Meta-Llama-3.1-8B-Instruct}
      --max-input-tokens ${MAX_INPUT_TOKENS:-8192}
      --max-total-tokens ${MAX_TOTAL_TOKENS:-8192}
      --num-shard 1
      --dtype bfloat16
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/data/.cache/huggingface
    ports:
      - "8080:8080"

  api:
    build:
      context: ..
      dockerfile: backend/Dockerfile
    environment:
      - DATABASE_URL=postgresql+psycopg2://visionbi:visionbi@db:5432/visionbi
      - MODEL_SERVER_URL=http://tgi:8080
      - DEFAULT_MODEL_ID=${MODEL_ID:-meta-llama/Meta-Llama-3.1-8B-Instruct}
      - CORS_ORIGIN=http://localhost:3000
      - TEMPERATURE=0.2
      - MAX_TOKENS=800
    depends_on:
      - db
      - tgi
    ports:
      - "8000:8000"

  web:
    build:
      context: ../frontend
      dockerfile: Dockerfile
    environment:
      - NEXT_PUBLIC_API_BASE=http://localhost:8000/api
    depends_on:
      - api
    ports:
      - "3000:3000"

volumes:
  db_data:
  hf_cache:


