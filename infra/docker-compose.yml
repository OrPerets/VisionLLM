services:
  db:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_USER: visionbi
      POSTGRES_PASSWORD: visionbi
      POSTGRES_DB: visionbi
    volumes:
      - db_data:/var/lib/postgresql/data
      - ./initdb.d:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"

  tgi:
    platform: linux/amd64
    image: ghcr.io/huggingface/text-generation-inference:2.0.4
    profiles: [gpu]
    env_file:
      - ./env.tgi
    environment:
      - MODEL_ID=${MODEL_ID:-TinyLlama/TinyLlama-1.1B-Chat-v1.0}
      - HF_HOME=/data/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    command: >-
      --model-id ${MODEL_ID:-openai/gpt-oss-20b}
      --port 8080
      --max-input-tokens ${MAX_INPUT_TOKENS:-8000}
      --max-total-tokens ${MAX_TOTAL_TOKENS:-8192}
      --num-shard 1
      --dtype float16
      --trust-remote-code
    shm_size: 1g
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 12
    volumes:
      - hf_cache:/data/.cache/huggingface
    ports:
      - "8080:8080"

  api:
    build:
      context: ..
      dockerfile: backend/Dockerfile
    environment:
      - DATABASE_URL=postgresql+psycopg2://visionbi:visionbi@db:5432/visionbi
      - MODEL_SERVER_URL=http://tgi:8080
      - DEFAULT_MODEL_ID=${OLLAMA_MODEL:-gpt-oss:20b}
      - CORS_ORIGIN=http://localhost:3000,http://localhost:3001
      - TEMPERATURE=0.2
      - MAX_TOKENS=800
      # Auth/env secrets are provided via env_file (see below)
      # Fast local POC path: use Ollama running on the host
      - MODEL_BACKEND=${MODEL_BACKEND:-ollama}
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gpt-oss:20b}
      - RAG_ENABLED=true
      - EMBEDDING_MODEL_ID=intfloat/e5-large-v2
      - RERANKER_MODEL_ID=BAAI/bge-reranker-v2-m3
    depends_on:
      - db
    ports:
      - "8000:8000"
    env_file:
      - ../backend/.env.api

  web:
    build:
      context: ../frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_BASE=http://localhost:8000/api
    environment:
      - NEXT_PUBLIC_API_BASE=http://localhost:8000/api
    depends_on:
      - api
    ports:
      - "3000:3000"

volumes:
  db_data:
  hf_cache:


